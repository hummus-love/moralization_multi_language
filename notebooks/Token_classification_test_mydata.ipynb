{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHLGJRxNJQh5"
   },
   "source": [
    "# Token classification (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:WARNING: skipping file '../data/All_Data/XMI_11/Wikipediadiskussionen-pos-BD-neu_ab_Zeile60-optimiert-CK-trimmed.xmi' due to XMLSyntaxError: xmlParseCharRef: invalid xmlChar value 30, line 27804, column 123159 (Wikipediadiskussionen-pos-BD-neu_ab_Zeile60-optimiert-CK-trimmed.xmi, line 27804)\n",
      "WARNING:root:WARNING: skipping file '../data/All_Data/XMI_11/Wikipediadiskussionen-neg-BD-neu-optimiert-CK.xmi' due to XMLSyntaxError: xmlParseCharRef: invalid xmlChar value 30, line 6765, column 5574 (Wikipediadiskussionen-neg-BD-neu-optimiert-CK.xmi, line 6765)\n"
     ]
    }
   ],
   "source": [
    "from moralization import input_data as inp\n",
    "from moralization import analyse as ae\n",
    "from moralization import spacy_model\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets \n",
    "import evaluate\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "torch.cuda.is_available()\n",
    "# import data as spacy doc and take it from there\n",
    "data_dir = \"../data/All_Data/XMI_11\"\n",
    "test_setup = spacy_model.SpacySetup(data_dir, working_dir=\"./test\")\n",
    "data_doc = test_setup.convert_data_to_spacy_doc()\n",
    "example_name = list(data_doc.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ']\n",
      "[0]\n",
      "['HMP05', '/', 'AUG.00228', 'Hamburger', 'Morgenpost', ',', '03.08.2005', ',', 'S.', '5', ';']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['ALG', 'II', 'ist', 'mit', 'der', 'Menschenwürde', 'vereinbar', '#', '#', '#']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['BERLIN']\n",
      "[0]\n",
      "['Das', 'Arbeitslosengeld', 'II', 'ist', 'nicht', 'so', 'niedrig', ',', 'dass', 'dadurch', 'die', 'grundgesetzlich', 'garantierte', 'Menschenwürde', 'verletzt', 'wird', ',', 'urteilte', 'das', 'Sozialgericht', 'Berlin', '.', '\\n']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ']\n",
      "[0]\n",
      "['HMP05', '/', 'AUG.00228', 'Hamburger', 'Morgenpost', ',', '03.08.2005', ',', 'S.', '5', ';']\n",
      "[0, -100, 0, 0, 0, -100, 0, -100, 0, 0, -100]\n",
      "['ALG', 'II', 'ist', 'mit', 'der', 'Menschenwürde', 'vereinbar', '#', '#', '#']\n",
      "[0, 0, 0, 0, 0, 0, 0, -100, -100, -100]\n",
      "['BERLIN']\n",
      "[0]\n",
      "['Das', 'Arbeitslosengeld', 'II', 'ist', 'nicht', 'so', 'niedrig', ',', 'dass', 'dadurch', 'die', 'grundgesetzlich', 'garantierte', 'Menschenwürde', 'verletzt', 'wird', ',', 'urteilte', 'das', 'Sozialgericht', 'Berlin', '.', '\\n']\n",
      "[2, 1, 1, 1, 1, 1, 1, -100, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1, 1, 1, 1, -100, 0]\n",
      "['Klägerin', 'war', 'eine', '55-jährige', 'Arbeitslose', ',', 'die', 'mit', 'ihrem', 'Mann', 'in', '\"', 'Bedarfsgemeinschaft', '\"', 'zusammenlebt', '.', '\\n']\n",
      "[0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, -100, 0, -100, 0, -100, 0]\n",
      "['Deswegen', 'bekommen', 'beide', 'pro', 'Kopf', 'nur', '311', 'Euro', 'ALG', 'II', 'statt', 'des', 'vollen', 'Satzes', 'von', '354', 'Euro', '.', '\\n']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0]\n",
      "['Arztkosten', 'seien', 'dadurch', 'ebenso', 'wenig', 'finanzierbar', 'wie', 'die', 'Teilnahme', 'am', 'gesellschaftlichen', 'Leben', ',', 'argumentierte', 'die', 'Klägerin', '.', '#', '#', '#']\n",
      "[2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1, 1, 1, -100, -100, -100, -100]\n",
      "['Dies', 'führe', 'zur', '#', 'Ausgrenzung', ',', '#', 'die', 'Menschenwürde', 'und', 'das', 'Sozialstaatsprinzip', 'seien', 'verletzt', '.']\n",
      "[2, 1, 1, -100, 1, -100, -100, 1, 1, 1, 1, 1, 1, 1, -100]\n",
      "['#', '#', '#']\n",
      "[-100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "sentence_list = [[token.text for token in sent] for sent in data_doc[example_name][\"train\"].sents]\n",
    "token_list = [[token for token in sent] for sent in data_doc[example_name][\"train\"].sents]\n",
    "# initialize nested label list to 0\n",
    "label_list = [[0 for token in sent] for sent in data_doc[example_name][\"train\"].sents]\n",
    "for i in range(0,5):\n",
    "    print(sentence_list[i])\n",
    "    print(label_list[i])\n",
    "# generate the labels based on the current list of tokens\n",
    "# now set all Moralisierung, Moralisierung Kontext, \n",
    "# Moralisierung explizit, Moralisierung interpretativ, Moralisierung Weltwissen to 1\n",
    "selected_labels = [\"Moralisierung\", \"Moralisierung Kontext\", \"Moralisierung Weltwissen\",\n",
    "                  \"Moralisierung explizit\", \"Moralisierung interpretativ\"]\n",
    "# create a list as long as tokens\n",
    "labels = [0 for token in data_doc[example_name][\"train\"]]\n",
    "for span in data_doc[example_name][\"train\"].spans[\"task1\"]:\n",
    "    if span.label_ in selected_labels:\n",
    "        labels[span.start+1:span.end] = [1] * (span.end-span.start)\n",
    "        # mark the beginning of a span with 2\n",
    "        labels[span.start] = 2\n",
    "        \n",
    "# labels now needs to be structured the same way as label_list\n",
    "# set the label at beginning of sentence to 2 if it is 1\n",
    "# also punctuation is included in the moralization label - we \n",
    "# definitely need to set those labels to -100\n",
    "j = 0\n",
    "for m in range(len(label_list)):\n",
    "    for i in range(len(label_list[m])):\n",
    "        label_list[m][i] = labels[j]\n",
    "        if i==0 and labels[j]==1:\n",
    "            label_list[m][i] = 2\n",
    "        if token_list[m][i].is_punct:\n",
    "            label_list[m][i] = -100\n",
    "        j = j+1\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(sentence_list[i])   \n",
    "    print(label_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Sentences  \\\n",
      "0  [                                             ...   \n",
      "1  [HMP05, /, AUG.00228, Hamburger, Morgenpost, ,...   \n",
      "2  [ALG, II, ist, mit, der, Menschenwürde, verein...   \n",
      "3                                           [BERLIN]   \n",
      "4  [Das, Arbeitslosengeld, II, ist, nicht, so, ni...   \n",
      "5  [Klägerin, war, eine, 55-jährige, Arbeitslose,...   \n",
      "6  [Deswegen, bekommen, beide, pro, Kopf, nur, 31...   \n",
      "7  [Arztkosten, seien, dadurch, ebenso, wenig, fi...   \n",
      "8  [Dies, führe, zur, #, Ausgrenzung, ,, #, die, ...   \n",
      "9                                          [#, #, #]   \n",
      "\n",
      "                                              Labels  \n",
      "0                                                [0]  \n",
      "1      [0, -100, 0, 0, 0, -100, 0, -100, 0, 0, -100]  \n",
      "2            [0, 0, 0, 0, 0, 0, 0, -100, -100, -100]  \n",
      "3                                                [0]  \n",
      "4  [2, 1, 1, 1, 1, 1, 1, -100, 1, 1, 1, 1, 1, 1, ...  \n",
      "5  [0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, -100, 0, ...  \n",
      "6  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "7  [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1, ...  \n",
      "8  [2, 1, 1, -100, 1, -100, -100, 1, 1, 1, 1, 1, ...  \n",
      "9                                 [-100, -100, -100]  \n"
     ]
    }
   ],
   "source": [
    "# at this point we can write the text into a csv to load into datasets\n",
    "# later it can be published as such on huggingface datasets\n",
    "# column heads are sentence, labels\n",
    "df = pd.DataFrame(zip(sentence_list,label_list), columns=[\"Sentences\", \"Labels\"])\n",
    "print(df.head(10))\n",
    "data_set = datasets.Dataset.from_pandas(df)\n",
    "# split in train test\n",
    "train_test_set = data_set.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-100, -100, -100],\n",
       " [-100, -100, -100],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  -100],\n",
       " [-100, -100, -100, -100],\n",
       " [0, 0, 0, -100, -100, -100],\n",
       " [-100, 0, 0, -100, 0, 0, -100],\n",
       " [0, -100, 0],\n",
       " [-100, -100, -100],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100],\n",
       " [0, 0, 0, 0, 0, 0, -100, 0, 0, -100, 0, 0, -100, 0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100],\n",
       " [2, 1, 1, 1, 1, 1, 1, 1, 1, -100],\n",
       " [0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, -100, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0],\n",
       " [-100, -100, -100],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0],\n",
       " [2, 1, 1, -100, 0, -100, 0, 0, -100],\n",
       " [-100, -100, -100],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0],\n",
       " [0, 0, 0, 0, 0, -100, 0, 0, -100, 0],\n",
       " [-100, -100, -100],\n",
       " [-100, 0, 0, 0, -100, 0, -100, 0, 0, -100],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0],\n",
       " [-100, -100, -100, -100, 0, -100, 0],\n",
       " [-100, -100, -100, -100, 1, 1, -100],\n",
       " [0, 0, 0, -100, 0, -100, 0, 0, 0, -100, 0, -100],\n",
       " [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1, 1, 1, 1, -100],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, -100, 0, -100],\n",
       " [-100, -100, -100],\n",
       " [0, 0, -100, 0, 0, 0, 0, -100, -100, 0, 0, 0, 0, 0, 0, -100, 0],\n",
       " [0, 0, 0, 0, 0, -100, 0],\n",
       " [-100, -100, -100],\n",
       " [-100, -100, -100],\n",
       " [0, 0, 0, -100, 0, -100, 0, 0, -100],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0],\n",
       " [0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -100,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -100,\n",
       "  1,\n",
       "  -100],\n",
       " [0, 0, 0, 0, 0, 0, 0, -100, 0],\n",
       " [-100, -100, -100, -100, 1, -100, 1, 1, -100, 1, -100, 1, 1, -100],\n",
       " [0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, -100, 0, -100, 0, -100, 0],\n",
       " [-100, -100, -100],\n",
       " [0, 0, 0, 0, 0, 0, 0, -100, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0],\n",
       " [0, 0, 0, -100, -100, -100],\n",
       " [0, -100, -100, -100],\n",
       " [-100, -100, -100, 0, 0, -100],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -100],\n",
       " [-100, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, -100, -100, 0],\n",
       " [0, 0, 0, 0, 0, 0, -100, -100, -100],\n",
       " [-100, -100, -100],\n",
       " [0, 0, 0, 0, -100, -100, 0, 0, 0, 0, 0, 0, 0, -100, -100, 0],\n",
       " [2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -100,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -100,\n",
       "  1],\n",
       " [0, 0, 0, 0, 0, 0, 0, -100, 0, 0, -100, 0, 0, 0, 0, -100, 0],\n",
       " [-100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100],\n",
       " [0, 0, 0, 0, 0, -100, 0, 0, 0, -100, -100],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0],\n",
       " [-100, -100, -100],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100],\n",
       " [0, 0, 0, 0, -100, 0, 0, 0, 0, 0, -100, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, -100, 0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -100,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -100,\n",
       "  1,\n",
       "  -100],\n",
       " [0, 0, -100, 0, -100, 0, 0, -100],\n",
       " [0, 0, 0, -100, 0, -100, 0, 0, -100],\n",
       " [-100, -100, -100],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100],\n",
       " [0, 0, 0, 0, -100, 0, 0, 0, 0, -100, 0, 0, -100, -100, 0],\n",
       " [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100],\n",
       " [0, 0, 0, -100],\n",
       " [0, 0, 0, 0, 0, 0, 0, -100, 0],\n",
       " [0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0],\n",
       " [0],\n",
       " [0, 0, 0, -100, 0, -100, 0, 0, -100],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, -100],\n",
       " [0, 0, -100, 0, 0, 0, 0, -100],\n",
       " [0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0],\n",
       " [0, 0, 0, -100, 0, -100, 0, 0, -100],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100],\n",
       " [-100, -100, -100, 1, 1, -100, 1, 1, 0, -100, 0, -100, 0, 0, -100],\n",
       " [-100, -100, -100],\n",
       " [-100, -100, -100],\n",
       " [0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, -100, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100],\n",
       " [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, 1],\n",
       " [-100, 0, 0, 0, 0, -100, 0, -100, 0, 0, 0, 0, 0, 0, 2, 1, -100, -100],\n",
       " [-100,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -100,\n",
       "  -100,\n",
       "  1,\n",
       "  1,\n",
       "  -100,\n",
       "  -100,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -100,\n",
       "  1,\n",
       "  -100,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -100],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, -100],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -100,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -100,\n",
       "  1,\n",
       "  -100,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -100,\n",
       "  -100],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100],\n",
       " [0, -100],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, -100, 0],\n",
       " [0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0],\n",
       " [0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_set[\"train\"][0:100][\"Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'E',\n",
       " '##s',\n",
       " 're',\n",
       " '##iche',\n",
       " 'a',\n",
       " '##ber',\n",
       " 'für',\n",
       " 'e',\n",
       " '##ine',\n",
       " 'men',\n",
       " '##schen',\n",
       " '##w',\n",
       " '##ü',\n",
       " '##rdi',\n",
       " '##ge',\n",
       " '\"',\n",
       " 'be',\n",
       " '##sche',\n",
       " '##iden',\n",
       " '##e',\n",
       " 'Le',\n",
       " '##ben',\n",
       " '##s',\n",
       " '##f',\n",
       " '##ü',\n",
       " '##hr',\n",
       " '##ung',\n",
       " '\"',\n",
       " 'no',\n",
       " '##ch',\n",
       " 'au',\n",
       " '##s',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(train_test_set[\"train\"][9][\"Sentences\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# label_list needs to be expanded to cover the new tokens\n",
    "# beginning of a span needs a different label than inside of a span\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label == 2:\n",
    "                label -= 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"Sentences\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"Labels\"]\n",
    "    new_labels = []\n",
    "    tokens = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        tokens.append(tokenized_inputs.tokens(i))\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "#     tokenized_inputs[\"tokens\"] = tokens\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f5b4f7446242208ba85ade2a777542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea75a1f9bc194cdea9bc1037d1f538c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = train_test_set.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=train_test_set[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-bP71JFwJQiF"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HW-b5rEPJQiF",
    "outputId": "9b423627-f808-4a4f-b34c-1995b2570df6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100],\n",
       "        [-100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zBqRd3DVJQiF",
    "outputId": "edcc88d8-ac03-4cba-9618-54c274fabb33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, -100, -100, -100, -100]\n",
      "[-100, -100, -100, -100, -100]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, -100, 0, 0, 0, 0, -100, 0, 0, 0, 0, -100, -100, -100]\n",
      "[-100, -100, -100, -100, -100, -100]\n",
      "[-100, 0, 0, 0, 0, 0, 0, -100, -100, -100, -100]\n",
      "[-100, -100, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, -100, -100]\n",
      "[-100, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, -100]\n",
      "[-100, -100, -100, -100, -100]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, -100, -100]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100]\n",
      "[-100, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, -100, -100]\n",
      "[-100, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100]\n",
      "[-100, -100, -100, -100, -100]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100]\n",
      "[-100, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, 0, 0, 0, 0, 0, -100, 0, 0, 0, -100, -100]\n",
      "[-100, -100, -100, -100, -100]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6lVyJf-oJQiG"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "r4j6oYEEJQiG",
    "outputId": "f1d0b881-2a9e-467e-ee77-6e111d81dcd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "# labels = [label_names[i] for i in labels]\n",
    "label_names = [\"0\", \"M\", \"M-BEG\"]\n",
    "labels = train_test_set[\"train\"][8][\"Labels\"]\n",
    "labels = [label_names[i] for i in labels if i != -100]\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KmM5hjfQJQiG",
    "outputId": "c65b86e2-647c-41f5-c748-4c9cb197b354"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iulusoy/miniconda3/envs/test-moralization/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/iulusoy/miniconda3/envs/test-moralization/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/iulusoy/miniconda3/envs/test-moralization/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/iulusoy/miniconda3/envs/test-moralization/lib/python3.9/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/iulusoy/miniconda3/envs/test-moralization/lib/python3.9/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'overall_precision': 0.0,\n",
       " 'overall_recall': 0.0,\n",
       " 'overall_f1': 0.0,\n",
       " 'overall_accuracy': 0.9333333333333333}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[2] = \"O\"\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6JE8RwzxJQiG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jYcU4mMcJQiG"
   },
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "om5rAJZTJQiG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PowPjfvEJQiH",
    "outputId": "ffb01359-68b9-4582-a4e3-a0ece49510a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RsEAYgiSJQiH"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"], collate_fn=data_collator, batch_size=8\n",
    "#     tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "c8V8EN5oJQiI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "bae3vQoIJQiI"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "d_ujG8ukJQiI"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "mvLRZdpyJQiI"
   },
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "6Ftz7qfAJQiI"
   },
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Wo1GW-txJQiI"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc04699aa6142468b2a3ce80fe810ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iulusoy/miniconda3/envs/test-moralization/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: M seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/iulusoy/miniconda3/envs/test-moralization/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: M-BEG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.7861788617886178}\n",
      "epoch 1: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.7861788617886178}\n",
      "epoch 2: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.7861788617886178}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "#         repo.push_to_hub(\n",
    "#             commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "K9uHoOVRJQiJ"
   },
   "outputs": [],
   "source": [
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "fUSoecaOJQiJ",
    "outputId": "f6614137-da44-4a92-d2df-f65577c5b92b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': '0',\n",
       "  'score': 0.9176057,\n",
       "  'word': 'Das Arbeitslosengeld ist nicht hoch genug da man ungleiche Standards propagiert.',\n",
       "  'start': 0,\n",
       "  'end': 80}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \".\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "token_classifier(\"Das Arbeitslosengeld ist nicht hoch genug da man ungleiche Standards propagiert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Token classification (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
