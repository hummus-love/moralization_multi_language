{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dae98cf",
   "metadata": {
    "id": "8dae98cf"
   },
   "source": [
    "# Notebook for transformers exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7F4Zm1i0XI-l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7F4Zm1i0XI-l",
    "outputId": "55cf49b7-aea4-4982-d736-f0e3665f374b"
   },
   "outputs": [],
   "source": [
    "# Please ignore this cell: extra install steps that are only executed when running the notebook on Google Colab\n",
    "# flake8-noqa-cell\n",
    "import os\n",
    "if 'google.colab' in str(get_ipython()) and not os.path.isdir('Test_Data'):\n",
    "    # we're running on colab and we haven't already downloaded the test data\n",
    "    # first install pinned version of setuptools (latest version doesn't seem to work with this package on colab)\n",
    "    !pip install setuptools==61 -qqq\n",
    "    # install the moralization package\n",
    "    !pip install git+https://github.com/ssciwr/moralization.git -qqq\n",
    "    # download test data sets\n",
    "    !wget https://github.com/ssciwr/moralization/archive/refs/heads/test_data.zip -q\n",
    "    !mkdir -p data && unzip -qq test_data.zip && mv -f moralization-test_data/*_Data ./data/. && rm -rf moralization-test_data test_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec233e",
   "metadata": {
    "id": "a8ec233e"
   },
   "outputs": [],
   "source": [
    "# flake8-noqa-cell\n",
    "from moralization import input_data as inp\n",
    "from moralization import analyse as ae\n",
    "from moralization import spacy_model\n",
    "import torch\n",
    "import pandas as pd\n",
    "import datasets \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6415e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07f6415e",
    "outputId": "a2c40de2-fc2e-44c7-e3af-37e1041498ba"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afMJChsoM6H5",
   "metadata": {
    "id": "afMJChsoM6H5"
   },
   "source": [
    "- Either pipeline (to simplify things) or load components manually - tokenizer (convert text to numbers), automodel with correct headers (ie classification) (model architecture and weights from pre-training)  \n",
    "- UNK (unknown) token for words not in vocab  \n",
    "- tokenizer is model-specific and contains certain algorithm and vocabulary for each model  \n",
    "- tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "- tokenizer(\"Using a Transformer network is simple\")  \n",
    "- tokenizer.save_pretrained(\"directory_on_my_computer\")\n",
    "- tokenization is followed by encoding\n",
    "- batches of text need to be padded, and attention mask indicates which tokens are padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2e666",
   "metadata": {
    "id": "smBvA5pPNJEA"
   },
   "source": [
    "# Second try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e007a87",
   "metadata": {},
   "source": [
    "## Data\n",
    "Eeach TOKEN needs to have one label in the training. So, if a sequence is classified as \"moralization\", then all the tokens in that sequence need to be assigned the label \"1\", and all other tokens \"0\". The beginning of a moralization, ie the first token is set to \"2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8-noqa-cell\n",
    "# import data as spacy doc and take it from there\n",
    "data_dir = \"../data/All_Data/XMI_11\"\n",
    "test_setup = spacy_model.SpacySetup(data_dir, working_dir=\"./test\")\n",
    "data_doc = test_setup.convert_data_to_spacy_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42abdeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# convert doc and span objects into list of tokens and labels\n",
    "# span.start returns the token id in the doc\n",
    "# data_doc = test_setup.doc_dict\n",
    "# print(data_doc.keys())\n",
    "example_name = list(data_doc.keys())[0]\n",
    "for span in data_doc[example_name][\"train\"].spans[\"task1\"]:\n",
    "    print(\"**********\")\n",
    "    print(span)\n",
    "    print(span.label_)\n",
    "    print(span.start)\n",
    "    print(data_doc[example_name][\"train\"][span.start], data_doc[example_name][\"train\"][span.end-1], \"mmm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb41213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and label\n",
    "# either list of sentences with list of tokens - here spacy needs to initialize with sentencizer\n",
    "# or list of instances with list of tokens\n",
    "# the instances must not be too long for this!\n",
    "# with sentences:\n",
    "sentence_list = [[token.text for token in sent] for sent in data_doc[example_name][\"train\"].sents]\n",
    "token_list = [[token for token in sent] for sent in data_doc[example_name][\"train\"].sents]\n",
    "# initialize nested label list to 0\n",
    "label_list = [[0 for token in sent] for sent in data_doc[example_name][\"train\"].sents]\n",
    "for i in range(0,5):\n",
    "    print(sentence_list[i])\n",
    "    print(label_list[i])\n",
    "\n",
    "# with instances:\n",
    "# find the pre-defined headlines and split there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3169b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the labels based on the current list of tokens\n",
    "# now set all Moralisierung, Moralisierung Kontext, \n",
    "# Moralisierung explizit, Moralisierung interpretativ, Moralisierung Weltwissen to 1\n",
    "selected_labels = [\"Moralisierung\", \"Moralisierung Kontext\", \"Moralisierung Weltwissen\",\n",
    "                  \"Moralisierung explizit\", \"Moralisierung interpretativ\"]\n",
    "# create a list as long as tokens\n",
    "labels = [0 for token in data_doc[example_name][\"train\"]]\n",
    "for span in data_doc[example_name][\"train\"].spans[\"task1\"]:\n",
    "    if span.label_ in selected_labels:\n",
    "        labels[span.start+1:span.end] = [1] * (span.end-span.start)\n",
    "        # mark the beginning of a span with 2\n",
    "        labels[span.start] = 2\n",
    "        \n",
    "# labels now needs to be structured the same way as label_list\n",
    "# set the label at beginning of sentence to 2 if it is 1\n",
    "# also punctuation is included in the moralization label - we \n",
    "# definitely need to set those labels to -100\n",
    "j = 0\n",
    "for m in range(len(label_list)):\n",
    "    for i in range(len(label_list[m])):\n",
    "        label_list[m][i] = labels[j]\n",
    "        if i==0 and labels[j]==1:\n",
    "            label_list[m][i] = 2\n",
    "        if token_list[m][i].is_punct:\n",
    "            label_list[m][i] = -100\n",
    "        j = j+1\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(sentence_list[i])   \n",
    "    print(label_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af845e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point we can write the text into a csv to load into datasets\n",
    "# later it can be published as such on huggingface datasets\n",
    "# column heads are sentence, labels\n",
    "df = pd.DataFrame(zip(sentence_list,label_list), columns=[\"Sentences\", \"Labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44257ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now load this into datasets\n",
    "data_set = datasets.Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78423840",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8347db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train test validation\n",
    "train_test_set = data_set.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0904ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_set[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8-noqa-cell\n",
    "model_name = \"xlm-roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can feed sentences into the tokenizer\n",
    "inputs = tokenizer(train_test_set[\"train\"][9][\"Sentences\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327e059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_set[\"train\"][9][\"Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ce5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list needs to be expanded to cover the new tokens\n",
    "# beginning of a span needs a different label than inside of a span\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label == 2:\n",
    "                label -= 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2112ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = inputs.word_ids()\n",
    "print(align_labels_with_tokens(train_test_set[\"train\"][9][\"Labels\"], word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5794f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_set[\"train\"][9][\"Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b95f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"Sentences\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"Labels\"]\n",
    "    new_labels = []\n",
    "    new_tokens = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "        new_tokens.append(tokenized_inputs.tokens(i))\n",
    "#         print(i, labels)\n",
    "#         print(words[i])\n",
    "#         print(tokenized_inputs.tokens(i))\n",
    "        \n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    tokenized_inputs[\"tokens\"] = new_tokens\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351acbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tokenize_and_align_labels(train_test_set[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1af50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = train_test_set.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=train_test_set[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e6de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets[\"train\"][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdc5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets[\"train\"][\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f07c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets[\"train\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator([tokenized_dataset[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410b3a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
