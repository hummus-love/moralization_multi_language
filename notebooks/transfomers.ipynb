{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dae98cf",
   "metadata": {
    "id": "8dae98cf"
   },
   "source": [
    "# Notebook for transformers exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7F4Zm1i0XI-l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7F4Zm1i0XI-l",
    "outputId": "55cf49b7-aea4-4982-d736-f0e3665f374b"
   },
   "outputs": [],
   "source": [
    "# Please ignore this cell: extra install steps that are only executed when running the notebook on Google Colab\n",
    "# flake8-noqa-cell\n",
    "import os\n",
    "if 'google.colab' in str(get_ipython()) and not os.path.isdir('Test_Data'):\n",
    "    # we're running on colab and we haven't already downloaded the test data\n",
    "    # first install pinned version of setuptools (latest version doesn't seem to work with this package on colab)\n",
    "    !pip install setuptools==61 -qqq\n",
    "    # install the moralization package\n",
    "    !pip install git+https://github.com/ssciwr/moralization.git -qqq\n",
    "    # download test data sets\n",
    "    !wget https://github.com/ssciwr/moralization/archive/refs/heads/test_data.zip -q\n",
    "    !mkdir -p data && unzip -qq test_data.zip && mv -f moralization-test_data/*_Data ./data/. && rm -rf moralization-test_data test_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec233e",
   "metadata": {
    "id": "a8ec233e"
   },
   "outputs": [],
   "source": [
    "# flake8-noqa-cell\n",
    "from moralization import input_data as inp\n",
    "from moralization import analyse as ae\n",
    "from moralization import spacy_model\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets \n",
    "import evaluate\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6415e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07f6415e",
    "outputId": "a2c40de2-fc2e-44c7-e3af-37e1041498ba"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afMJChsoM6H5",
   "metadata": {
    "id": "afMJChsoM6H5"
   },
   "source": [
    "- Either pipeline (to simplify things) or load components manually - tokenizer (convert text to numbers), automodel with correct headers (ie classification) (model architecture and weights from pre-training)  \n",
    "- UNK (unknown) token for words not in vocab  \n",
    "- tokenizer is model-specific and contains certain algorithm and vocabulary for each model  \n",
    "- tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "- tokenizer(\"Using a Transformer network is simple\")  \n",
    "- tokenizer.save_pretrained(\"directory_on_my_computer\")\n",
    "- tokenization is followed by encoding\n",
    "- batches of text need to be padded, and attention mask indicates which tokens are padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2e666",
   "metadata": {
    "id": "smBvA5pPNJEA"
   },
   "source": [
    "# Second try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e007a87",
   "metadata": {},
   "source": [
    "## Data\n",
    "Eeach TOKEN needs to have one label in the training. So, if a sequence is classified as \"moralization\", then all the tokens in that sequence need to be assigned the label \"1\", and all other tokens \"0\". The beginning of a moralization, ie the first token is set to \"2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8-noqa-cell\n",
    "# import data as spacy doc and take it from there\n",
    "data_dir = \"../data/All_Data/XMI_11\"\n",
    "test_setup = spacy_model.SpacySetup(data_dir, working_dir=\"./test\")\n",
    "data_doc = test_setup.convert_data_to_spacy_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42abdeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# convert doc and span objects into list of tokens and labels\n",
    "# span.start returns the token id in the doc\n",
    "# data_doc = test_setup.doc_dict\n",
    "# print(data_doc.keys())\n",
    "example_name = list(data_doc.keys())[0]\n",
    "# for span in data_doc[example_name][\"train\"].spans[\"task1\"]:\n",
    "#     print(\"**********\")\n",
    "#     print(span)\n",
    "#     print(span.label_)\n",
    "#     print(span.start)\n",
    "#     print(data_doc[example_name][\"train\"][span.start], data_doc[example_name][\"train\"][span.end-1], \"mmm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb41213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and label\n",
    "# either list of sentences with list of tokens - here spacy needs to initialize with sentencizer\n",
    "# or list of instances with list of tokens\n",
    "# the instances must not be too long for this!\n",
    "# with sentences:\n",
    "sentence_list = [[token.text for token in sent] for sent in data_doc[example_name][\"train\"].sents]\n",
    "token_list = [[token for token in sent] for sent in data_doc[example_name][\"train\"].sents]\n",
    "# initialize nested label list to 0\n",
    "label_list = [[0 for token in sent] for sent in data_doc[example_name][\"train\"].sents]\n",
    "for i in range(0,5):\n",
    "    print(sentence_list[i])\n",
    "    print(label_list[i])\n",
    "\n",
    "# with instances:\n",
    "# find the pre-defined headlines and split there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3169b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the labels based on the current list of tokens\n",
    "# now set all Moralisierung, Moralisierung Kontext, \n",
    "# Moralisierung explizit, Moralisierung interpretativ, Moralisierung Weltwissen to 1\n",
    "selected_labels = [\"Moralisierung\", \"Moralisierung Kontext\", \"Moralisierung Weltwissen\",\n",
    "                  \"Moralisierung explizit\", \"Moralisierung interpretativ\"]\n",
    "# create a list as long as tokens\n",
    "labels = [0 for token in data_doc[example_name][\"train\"]]\n",
    "for span in data_doc[example_name][\"train\"].spans[\"task1\"]:\n",
    "    if span.label_ in selected_labels:\n",
    "        labels[span.start+1:span.end] = [1] * (span.end-span.start)\n",
    "        # mark the beginning of a span with 2\n",
    "        labels[span.start] = 2\n",
    "        \n",
    "# labels now needs to be structured the same way as label_list\n",
    "# set the label at beginning of sentence to 2 if it is 1\n",
    "# also punctuation is included in the moralization label - we \n",
    "# definitely need to set those labels to -100\n",
    "j = 0\n",
    "for m in range(len(label_list)):\n",
    "    for i in range(len(label_list[m])):\n",
    "        label_list[m][i] = labels[j]\n",
    "        if i==0 and labels[j]==1:\n",
    "            label_list[m][i] = 2\n",
    "        if token_list[m][i].is_punct:\n",
    "            label_list[m][i] = -100\n",
    "        j = j+1\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(sentence_list[i])   \n",
    "    print(label_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc424a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point we can write the text into a csv to load into datasets\n",
    "# later it can be published as such on huggingface datasets\n",
    "# column heads are sentence, labels\n",
    "df = pd.DataFrame(zip(sentence_list,label_list), columns=[\"Sentences\", \"Labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)\n",
    "# Problem is the sentence split - does not get token 101 / 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d827ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now load this into datasets\n",
    "data_set = datasets.Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4635aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80842c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9903b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train test validation\n",
    "train_test_set = data_set.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82cc73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_set[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8-noqa-cell\n",
    "# model_name = \"xlm-roberta-large\"\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can feed sentences into the tokenizer\n",
    "inputs = tokenizer(train_test_set[\"train\"][9][\"Sentences\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_set[\"train\"][9][\"Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ce5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list needs to be expanded to cover the new tokens\n",
    "# beginning of a span needs a different label than inside of a span\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label == 2:\n",
    "                label -= 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2112ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = inputs.word_ids()\n",
    "print(align_labels_with_tokens(train_test_set[\"train\"][9][\"Labels\"], word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5794f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_set[\"train\"][9][\"Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b95f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"Sentences\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"Labels\"]\n",
    "    new_labels = []\n",
    "    tokens = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        tokens.append(tokenized_inputs.tokens(i))\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "#     tokenized_inputs[\"tokens\"] = tokens\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1af50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = train_test_set.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=train_test_set[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e6de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets[\"train\"][\"input_ids\"][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35424101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets[\"train\"][\"tokens\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdc5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets[\"train\"][\"attention_mask\"][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef4892",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets[\"train\"][\"labels\"][5:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "# batch = data_collator(tokenized_datasets[\"train\"])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410b3a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that data has been padded to same length\n",
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4778b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ae615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [[label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb253580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_names = [\"0\", \"M\", \"M-BEG\"]\n",
    "# labels = tokenized_datasets[\"train\"][8][\"labels\"]\n",
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "label_names = ner_feature.feature.names\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "print(labels)\n",
    "labels = [label_names[i] for i in labels if i != -100]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de703a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[2] = \"0\"\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85672e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the label names\n",
    "# label_names = [\"0\", \"M\", \"M-BEG\"]\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e4eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,id2label=id2label,label2id=label2id,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc72d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313eb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"], collate_fn=data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a161d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108fc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "312/39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caacc547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        for key in batch.keys():\n",
    "            print(key)\n",
    "            print(batch[key].shape)\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fdb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with standard data\n",
    "raw_datasets = datasets.load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465cf64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8de784",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15681451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a157c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58556fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ac0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22786a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e632e899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
